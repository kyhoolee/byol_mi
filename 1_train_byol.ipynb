{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# from modules import BYOL\n",
    "# from modules.transformations import TransformsSimCLR\n",
    "import copy\n",
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper functions\n",
    "\n",
    "# distributed training\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def mkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "\n",
    "class TransformsSimCLR:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module that transforms any given data example randomly \n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        s = 1\n",
    "        color_jitter = torchvision.transforms.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        self.train_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.RandomResizedCrop(size=size),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
    "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
    "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(size=size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1: tensor([[-0.1128, -2.1092,  0.2635,  0.6669, -0.6763, -0.0313,  0.6924,  0.4403,\n",
      "          1.7536, -0.7036,  0.1907,  0.5174],\n",
      "        [-2.7859, -1.3126, -0.0597, -0.4849,  1.1314, -0.3457, -0.4029, -0.4896,\n",
      "         -1.6431, -0.4440, -0.7539,  2.0112],\n",
      "        [ 0.1758,  0.2646,  1.6288,  1.5116, -1.1957,  0.8327,  2.1143, -1.4118,\n",
      "         -0.7418, -1.4383, -1.0075,  1.4252],\n",
      "        [ 0.8099,  0.0328, -0.4675,  1.3027, -0.4971,  1.3294,  0.6015, -0.6023,\n",
      "         -0.4650, -0.8172,  0.6054, -0.5021]])\n",
      "Input2: tensor([[-0.5113, -0.8285,  1.0173, -1.1441,  0.8226,  0.9513,  0.7124,  2.0193,\n",
      "          0.6536, -0.1931,  0.1525, -0.1074],\n",
      "        [ 1.1626, -0.6248,  0.7782,  1.8841,  0.3065, -1.4666,  1.8618,  0.1381,\n",
      "         -0.0877,  1.0392, -0.8166, -0.0641],\n",
      "        [ 0.1579,  0.8694,  0.5434,  0.7253,  0.2851, -1.7096,  0.1727, -0.6824,\n",
      "          0.2591,  2.0393,  1.8917,  1.0290],\n",
      "        [-1.1561,  0.1702,  0.2000, -0.3092,  1.0136, -0.5957, -0.2376, -0.7837,\n",
      "          0.3147, -0.1792,  0.6673, -0.3833]])\n",
      "Cosine: CosineSimilarity()\n",
      "Output: tensor([ 0.3372, -0.1988, -0.1073, -0.3329])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input1 = torch.randn(4, 12)\n",
    "input2 = torch.randn(4, 12)\n",
    "print('Input1:', input1)\n",
    "print('Input2:', input2)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "print('Cosine:', cos)\n",
    "output = cos(input1, input2)\n",
    "print('Output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([-1.4904,  0.3065])\n",
      "Output:  tensor([0.2032, 0.8581])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softplus()\n",
    "input = torch.randn(2)\n",
    "print('Input: ', input)\n",
    "output = m(input)\n",
    "print('Output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0.7117, -0.9927, -0.2839, -1.3336,  0.1674,  0.0136, -0.4612, -0.0393,\n",
      "          2.0642,  0.3666,  1.1870,  1.0393],\n",
      "        [-0.6244, -1.0016,  0.7983,  0.9031,  0.3688, -1.1939, -0.0909, -0.1809,\n",
      "          0.6790,  0.0811,  1.3697, -0.3409],\n",
      "        [ 0.5057,  0.9305,  1.0444,  0.6630,  0.2778, -0.7578, -0.3167,  1.6050,\n",
      "          1.6239, -1.8701, -0.3561,  0.0958],\n",
      "        [-0.4307, -0.3881,  1.6332,  1.8315, -0.5780,  0.4772, -0.3420,  0.3544,\n",
      "         -0.6579, -0.2821,  0.2395,  0.4580]])\n",
      "Y:  tensor([[-0.4307, -0.3881,  1.6332,  1.8315, -0.5780,  0.4772, -0.3420,  0.3544,\n",
      "         -0.6579, -0.2821,  0.2395,  0.4580],\n",
      "        [ 0.7117, -0.9927, -0.2839, -1.3336,  0.1674,  0.0136, -0.4612, -0.0393,\n",
      "          2.0642,  0.3666,  1.1870,  1.0393],\n",
      "        [-0.6244, -1.0016,  0.7983,  0.9031,  0.3688, -1.1939, -0.0909, -0.1809,\n",
      "          0.6790,  0.0811,  1.3697, -0.3409],\n",
      "        [ 0.5057,  0.9305,  1.0444,  0.6630,  0.2778, -0.7578, -0.3167,  1.6050,\n",
      "          1.6239, -1.8701, -0.3561,  0.0958]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 12)\n",
    "print('X: ', x)\n",
    "dim = 0\n",
    "rotate = [x.shape[dim]-1] + list(range(0, x.shape[dim]-1))\n",
    "# print(tuple(rotate))\n",
    "# torch.permute(x, (2, 0, 1)).size()\n",
    "# y = torch.permute(x, rotate)\n",
    "\n",
    "\n",
    "y = torch.index_select(x, dim, torch.LongTensor(rotate))\n",
    "print('Y: ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val\n",
    "\n",
    "\n",
    "def flatten(t):\n",
    "    return t.reshape(t.shape[0], -1)\n",
    "\n",
    "\n",
    "'''\n",
    "Write code for singleton instance attribute \n",
    "Call initialize function only once - iff not exist\n",
    "Then use the initialized instance \n",
    "'''\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return inner_fn\n",
    "\n",
    "\n",
    "# loss fn\n",
    "def soft_cos(x,y, temperature=0.1):\n",
    "    cos = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "    soft = nn.Softplus()\n",
    "    result = soft(-1/temperature * cos(x,y))\n",
    "    return result\n",
    "\n",
    "def loss_fn(x, y, gpu, temperature=0.1):\n",
    "    '''\n",
    "    x - from online \n",
    "    y - from target \n",
    "    '''\n",
    "    cos = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "    soft = nn.Softplus()\n",
    "    positive = soft_cos(x,y, temperature)\n",
    "    \n",
    "    \n",
    "    dim = 0\n",
    "    rotate = [y.shape[dim]-1] + list(range(0, y.shape[dim]-1))\n",
    "    rotate = torch.LongTensor(rotate)\n",
    "    \n",
    "    if x.is_cuda:\n",
    "        rotate = rotate.cuda(gpu)\n",
    "    \n",
    "    print('X:', x.is_cuda, x.get_device())\n",
    "    print('Y:', y.is_cuda, y.get_device())\n",
    "    print('Rotate:', rotate.is_cuda, rotate.get_device())\n",
    "    \n",
    "    y = torch.index_select(y, dim, rotate)\n",
    "    negative = soft_cos(x,y, temperature)\n",
    "    \n",
    "    result = positive - negative\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    \n",
    "def byol_loss(x,y):\n",
    "    print('Output shape: ', x.shape, y.shape)\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    \n",
    "    result = 2 - 2 * (x * y).sum(dim=-1)\n",
    "    print('Loss: ', result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# augmentation utils\n",
    "\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "# exponential moving average\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        # EMA simple updating function \n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(\n",
    "        current_model.parameters(), ma_model.parameters()\n",
    "    ):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "\n",
    "# MLP class for projector and predictor\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, projection_size, hidden_size=4096):\n",
    "        super().__init__()\n",
    "        # Projector still go through several step of input - hidden - output \n",
    "        # Not simply a direct linear mapping \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, projection_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# a wrapper class for the base neural network\n",
    "# will manage the interception of the hidden layer output\n",
    "# and pipe it into the projecter and predictor nets\n",
    "\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    def __init__(self, net, projection_size, projection_hidden_size, layer=-2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Backbone network \n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        # 2. Projector - map from backbone output to projection output \n",
    "        self.projector = None\n",
    "        self.projection_size = projection_size\n",
    "        self.projection_hidden_size = projection_hidden_size\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = flatten(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f\"hidden layer ({self.layer}) not found\"\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    @singleton(\"projector\")\n",
    "    def _get_projector(self, hidden):\n",
    "        _, dim = hidden.shape\n",
    "        # Projector simply is a MLP to map from backbone output to projection output \n",
    "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
    "        return projector.to(hidden)\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        \n",
    "        # 1. Not really understand why need to do this to get representation output \n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        _ = self.net(x)\n",
    "\n",
    "        # 2. Using hook and a lot of thing to get hidden - instead of directory assign from backbone net\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f\"hidden layer {self.layer} never emitted an output\"\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Get representation from backbone net \n",
    "        representation = self.get_representation(x)\n",
    "\n",
    "        # 2. Get projection from projector \n",
    "        # 2.1. Get projector - why need to intialize new projector for every forward step ???\n",
    "        # And why need to use singleton here ??? \n",
    "        projector = self._get_projector(representation)\n",
    "        # 2.2. Get projection from projector \n",
    "        projection = projector(representation)\n",
    "        return projection\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        image_size,\n",
    "        device,\n",
    "        hidden_layer=-2,\n",
    "        projection_size=256,\n",
    "        projection_hidden_size=4096,\n",
    "        augment_fn=None,\n",
    "        moving_average_decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # 1. Online encoder \n",
    "        # Backbone network model \n",
    "        self.online_encoder = NetWrapper(\n",
    "            net, projection_size, projection_hidden_size, layer=hidden_layer\n",
    "        )\n",
    "\n",
    "        # 2. Target encoder \n",
    "        # Backbone network model - Current None - will be copy from online encoder \n",
    "        self.target_encoder = None\n",
    "        # 2.1. Target parameter updater - Exponential moving average \n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "\n",
    "        # 3. Online preditor \n",
    "        # Using to predict online output to target output \n",
    "        self.online_predictor = MLP(\n",
    "            projection_size, projection_size, projection_hidden_size\n",
    "        )\n",
    "\n",
    "        # send a mock image tensor to instantiate singleton parameters\n",
    "        self.forward(torch.randn(2, 3, image_size, image_size), torch.randn(2, 3, image_size, image_size))\n",
    "\n",
    "\n",
    "    @singleton(\"target_encoder\")\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        return target_encoder\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.target_encoder\n",
    "        self.target_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert (\n",
    "            self.target_encoder is not None\n",
    "        ), \"target encoder has not been created yet\"\n",
    "\n",
    "        # EMA update for target parameters using current target parameters and online parameters \n",
    "        update_moving_average(\n",
    "            self.target_ema_updater, self.target_encoder, self.online_encoder\n",
    "        )\n",
    "\n",
    "    def forward(self, image_one, image_two):\n",
    "\n",
    "        # Do forward with 2 augmented version of 1 image \n",
    "        # Forward with both online and target and take average of loss \n",
    "        # A little bit different from original formula - but still the same mearning \n",
    "\n",
    "        # 1. Online output - do normally through encoder and predictor \n",
    "        online_proj_one = self.online_encoder(image_one)\n",
    "        online_proj_two = self.online_encoder(image_two)\n",
    "\n",
    "        online_pred_one = self.online_predictor(online_proj_one)\n",
    "        online_pred_two = self.online_predictor(online_proj_two)\n",
    "\n",
    "        # 2. Target output - do with no grad - encoder no update by backward \n",
    "        with torch.no_grad():\n",
    "            # Why at each forward step - target encoder is updated by same as online encoder ???\n",
    "            # And why need to use singleton here ??? \n",
    "            target_encoder = self._get_target_encoder()\n",
    "            target_proj_one = target_encoder(image_one)\n",
    "            target_proj_two = target_encoder(image_two)\n",
    "\n",
    "        # 3. Calculate loss - target output detach from backward \n",
    "        loss_one = loss_fn(online_pred_one, target_proj_two.detach(), self.device)\n",
    "        loss_two = loss_fn(online_pred_two, target_proj_one.detach(), self.device)\n",
    "\n",
    "        loss = loss_one + loss_two\n",
    "        return -math.log(4) + loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(gpu, args):\n",
    "\n",
    "    # 0. Initialize distributed GPU training \n",
    "    rank = args.nr * args.gpus + gpu\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=args.world_size)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    # 1. dataset\n",
    "\n",
    "    # Using pytorch datasets with custom-transform to generate augmentation \n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        args.dataset_dir,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size), # paper 224\n",
    "    )\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset, num_replicas=args.world_size, rank=rank\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "\n",
    "    # 2. model\n",
    "    if args.resnet_version == \"resnet18\":\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "    elif args.resnet_version == \"resnet50\":\n",
    "        resnet = models.resnet50(pretrained=False)\n",
    "    else:\n",
    "        raise NotImplementedError(\"ResNet not implemented\")\n",
    "\n",
    "    # 2.1. BYOL model \n",
    "    model = BYOL(resnet, image_size=args.image_size, device=gpu, hidden_layer=\"avgpool\")\n",
    "    model = model.cuda(gpu)\n",
    "\n",
    "    mkdir(args.train_dir)\n",
    "\n",
    "    # 2.2. Distributed data parallel\n",
    "    model = DDP(model, device_ids=[gpu], find_unused_parameters=True)\n",
    "\n",
    "    # 3. optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 4. TensorBoard writer\n",
    "\n",
    "    if gpu == 0:\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "    # 5. Solver\n",
    "    global_step = 0\n",
    "\n",
    "    # Training loop \n",
    "    for epoch in range(args.num_epochs):\n",
    "        metrics = defaultdict(list)\n",
    "        for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "            print('Step: ', step, ', batch-shape: ', x_i.shape, x_j.shape)\n",
    "\n",
    "            # Get 2 augmented samples from same samples \n",
    "            # Logic in dataset \n",
    "            x_i = x_i.cuda(non_blocking=True)\n",
    "            x_j = x_j.cuda(non_blocking=True)\n",
    "            \n",
    "            print('X_i: ', x_i.is_cuda, x_i.get_device())\n",
    "            print('X_j: ', x_j.is_cuda, x_j.get_device())\n",
    "\n",
    "            # Calculate loss \n",
    "            # Logic in detail BYOL model \n",
    "            loss = model(x_i, x_j)\n",
    "\n",
    "            # Optimize and backward \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update by exponential average moving \n",
    "            # Logic in detail BYOL model \n",
    "            model.module.update_moving_average()  # update moving average of target encoder\n",
    "\n",
    "            if step % 10 == 0 and gpu == 0:\n",
    "                print(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\")\n",
    "\n",
    "            if gpu == 0:\n",
    "                writer.add_scalar(\"Loss/train_step\", loss, global_step)\n",
    "                metrics[\"Loss/train\"].append(loss.item())\n",
    "                global_step += 1\n",
    "                \n",
    "            break\n",
    "        break\n",
    "\n",
    "        if gpu == 0:\n",
    "            # write metrics to TensorBoard\n",
    "            for k, v in metrics.items():\n",
    "                writer.add_scalar(k, np.array(v).mean(), epoch)\n",
    "\n",
    "            if epoch % args.checkpoint_epochs == 0:\n",
    "                if gpu == 0:\n",
    "                    now = datetime.now()\n",
    "                    print(now)\n",
    "                    print(f\"Saving model at epoch {epoch}\")\n",
    "                    torch.save(resnet.state_dict(), f\"{args.train_dir}/model-{epoch}.pt\")\n",
    "\n",
    "                # let other workers wait until model is finished\n",
    "                # dist.barrier()\n",
    "\n",
    "    # save your improved network\n",
    "    if gpu == 0:\n",
    "        torch.save(resnet.state_dict(), f\"{args.train_dir}/model-final.pt\")\n",
    "\n",
    "    cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "\n",
    "args = DotMap()\n",
    "\n",
    "args.batch_size=192\n",
    "args.checkpoint_epochs=5\n",
    "args.dataset_dir='./datasets'\n",
    "args.gpus=4\n",
    "args.image_size=224\n",
    "args.learning_rate=0.0003\n",
    "args.nodes=1\n",
    "args.nr=0\n",
    "args.num_epochs=100\n",
    "args.num_workers=8\n",
    "args.resnet_version='resnet18'\n",
    "args.train_dir='./train_dir_mi_202112-17'\n",
    "args.world_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "X: False -1\n",
      "Y: False -1\n",
      "Rotate: False -1\n",
      "X: False -1\n",
      "Y: False -1\n",
      "Rotate: False -1\n",
      "Step:  0 , batch-shape:  torch.Size([192, 3, 224, 224]) torch.Size([192, 3, 224, 224])\n",
      "X_i:  True 0\n",
      "X_j:  True 0\n",
      "X: True 0\n",
      "Y: True 0\n",
      "Rotate: True 0\n",
      "X: True 0\n",
      "Y: True 0\n",
      "Rotate: True 0\n",
      "Step [0/260]:\tLoss: -1.432499885559082\n"
     ]
    }
   ],
   "source": [
    "# Master address for distributed data parallel\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"8011\"\n",
    "\n",
    "# Initialize the process and join up with the other processes.\n",
    "# This is “blocking,” meaning that no process will continue until all processes have joined.\n",
    "# mp.spawn(main, args=(args,), nprocs=args.gpus, join=True)\n",
    "main(0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files already downloaded and verified\n",
    "# Step:  0 , batch-shape:  torch.Size([192, 3, 224, 224]) torch.Size([192, 3, 224, 224])\n",
    "# Step [0/260]:\tLoss: 3.972233295440674\n",
    "\n",
    "# Fix resnet18 - cifar10 accuracy \n",
    "# https://hd10.dev/posts/experiments_cifar10_part1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "X: False -1\n",
      "Y: False -1\n",
      "Rotate: False -1\n",
      "X: False -1\n",
      "Y: False -1\n",
      "Rotate: False -1\n",
      "Step:  0 , batch-shape:  torch.Size([192, 3, 224, 224]) torch.Size([192, 3, 224, 224])\n",
      "X_i:  True 0\n",
      "X_j:  True 0\n",
      "X: True 0\n",
      "Y: True 0\n",
      "Rotate: True 0\n",
      "X: True 0\n",
      "Y: True 0\n",
      "Rotate: True 0\n",
      "Step [0/260]:\tLoss: -1.432499885559082\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "\n",
    "# 0. Initialize distributed GPU training \n",
    "rank = args.nr * args.gpus + gpu\n",
    "dist.init_process_group(\"nccl\", rank=rank, world_size=args.world_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.set_device(gpu)\n",
    "\n",
    "# 1. dataset\n",
    "\n",
    "# Using pytorch datasets with custom-transform to generate augmentation \n",
    "train_dataset = datasets.CIFAR10(\n",
    "    args.dataset_dir,\n",
    "    download=True,\n",
    "    transform=TransformsSimCLR(size=args.image_size), # paper 224\n",
    ")\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=args.world_size, rank=rank\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "\n",
    "# 2. model\n",
    "if args.resnet_version == \"resnet18\":\n",
    "    resnet = models.resnet18(pretrained=False)\n",
    "elif args.resnet_version == \"resnet50\":\n",
    "    resnet = models.resnet50(pretrained=False)\n",
    "else:\n",
    "    raise NotImplementedError(\"ResNet not implemented\")\n",
    "\n",
    "# 2.1. BYOL model \n",
    "model = BYOL(resnet, image_size=args.image_size, device=gpu, hidden_layer=\"avgpool\")\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "mkdir(args.train_dir)\n",
    "\n",
    "# 2.2. Distributed data parallel\n",
    "model = DDP(model, device_ids=[gpu], find_unused_parameters=True)\n",
    "\n",
    "# 3. optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# 4. TensorBoard writer\n",
    "\n",
    "if gpu == 0:\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "# 5. Solver\n",
    "global_step = 0\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(args.num_epochs):\n",
    "    metrics = defaultdict(list)\n",
    "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "        print('Step: ', step, ', batch-shape: ', x_i.shape, x_j.shape)\n",
    "\n",
    "        # Get 2 augmented samples from same samples \n",
    "        # Logic in dataset \n",
    "        x_i = x_i.cuda(non_blocking=True)\n",
    "        x_j = x_j.cuda(non_blocking=True)\n",
    "\n",
    "        print('X_i: ', x_i.is_cuda, x_i.get_device())\n",
    "        print('X_j: ', x_j.is_cuda, x_j.get_device())\n",
    "\n",
    "        # Calculate loss \n",
    "        # Logic in detail BYOL model \n",
    "        loss = model(x_i, x_j)\n",
    "\n",
    "        # Optimize and backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update by exponential average moving \n",
    "        # Logic in detail BYOL model \n",
    "        model.module.update_moving_average()  # update moving average of target encoder\n",
    "\n",
    "        if step % 10 == 0 and gpu == 0:\n",
    "            print(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\")\n",
    "\n",
    "        if gpu == 0:\n",
    "            writer.add_scalar(\"Loss/train_step\", loss, global_step)\n",
    "            metrics[\"Loss/train\"].append(loss.item())\n",
    "            global_step += 1\n",
    "\n",
    "        break\n",
    "    break\n",
    "\n",
    "    if gpu == 0:\n",
    "        # write metrics to TensorBoard\n",
    "        for k, v in metrics.items():\n",
    "            writer.add_scalar(k, np.array(v).mean(), epoch)\n",
    "\n",
    "        if epoch % args.checkpoint_epochs == 0:\n",
    "            if gpu == 0:\n",
    "                now = datetime.now()\n",
    "                print(now)\n",
    "                print(f\"Saving model at epoch {epoch}\")\n",
    "                torch.save(resnet.state_dict(), f\"{args.train_dir}/model-{epoch}.pt\")\n",
    "\n",
    "            # let other workers wait until model is finished\n",
    "            # dist.barrier()\n",
    "\n",
    "# save your improved network\n",
    "if gpu == 0:\n",
    "    torch.save(resnet.state_dict(), f\"{args.train_dir}/model-final.pt\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedDataParallel(\n",
      "  (module): BYOL(\n",
      "    (online_encoder): NetWrapper(\n",
      "      (net): ResNet(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "        (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "      )\n",
      "      (projector): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (online_predictor): MLP(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=4096, bias=True)\n",
      "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (target_encoder): NetWrapper(\n",
      "      (net): ResNet(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "        (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "      )\n",
      "      (projector): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL(\n",
      "  (online_encoder): NetWrapper(\n",
      "    (net): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (projector): MLP(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): MLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=4096, bias=True)\n",
      "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (target_encoder): NetWrapper(\n",
      "    (net): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (projector): MLP(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=4096, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this instead of nn.Conv2d at all places\n",
    "class WeightStdConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight\n",
    "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
    "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
    "        weight = weight - weight_mean\n",
    "        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n",
    "        weight = weight / std.expand_as(weight)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(resnet.conv1)\n",
    "\n",
    "def cifar10_resnet18(resnet):\n",
    "    resnet.conv1 = Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byol_tut",
   "language": "python",
   "name": "byol_tut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
